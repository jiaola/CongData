{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this page [https://nbviewer.jupyter.org/github/rare-technologies/gensim/blob/develop/docs/notebooks/atmodel_tutorial.ipynb] for reference. \n",
    "Extracting data from XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212\n"
     ]
    }
   ],
   "source": [
    "import os, re, glob\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_text(file):\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "    namespaces = {'dc': 'http://purl.org/dc/elements/1.1/'}\n",
    "    title = root.find('.//form/official-title').text\n",
    "    return title + ' ' + ''.join(root.find('.//legis-body').itertext())\n",
    "\n",
    "def identifier(filename):\n",
    "    m = re.search('.*-(\\d+s\\d+).*', filename)\n",
    "    return m.group(1)\n",
    "    \n",
    "docs = []\n",
    "ids = []\n",
    "files = glob.glob('data/bills/s/BILLS-115*.xml')\n",
    "for file in files:    \n",
    "    id = identifier(file)\n",
    "    if 'eah.xml' in file or id in ids:\n",
    "        continue\n",
    "    ids.append(identifier(file))\n",
    "    txt = extract_text(file)\n",
    "    txt = re.sub('\\s+', ' ', txt).strip()\n",
    "    docs.append(txt)  \n",
    "print(len(docs))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creating sponsor-bill mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_sponsors(file):\n",
    "    tree = ET.parse(file)\n",
    "    sponsors = []\n",
    "    root = tree.getroot()\n",
    "    sponsors = [n.text for n in root.findall('.//sponsors/item/bioguideId')]\n",
    "    sponsors.extend([n.text for n in root.findall(\".//cosponsors/item/bioguideId\")])\n",
    "    return sponsors\n",
    "\n",
    "sponsor2docs = dict() \n",
    "status_files = os.listdir('data/status/s')\n",
    "for file in status_files:\n",
    "    sponsors = extract_sponsors('data/status/s/' + file)\n",
    "    id = identifier(file)\n",
    "    if not id in ids:\n",
    "        continue\n",
    "    for sponsor in sponsors:\n",
    "        if not sponsor2docs.get(sponsor):\n",
    "            # This is a new sponsor.\n",
    "            sponsor2docs[sponsor] = []\n",
    "        sponsor2docs[sponsor].append(id)\n",
    "\n",
    "# Use an integer ID in author2doc, instead of the IDs provided in the NIPS dataset.\n",
    "# Mapping from ID of document in NIPS datast, to an integer ID.\n",
    "id_dict = dict(zip(ids, range(len(ids))))\n",
    "# Replace NIPS IDs by integer IDs.\n",
    "for a, a_doc_ids in sponsor2docs.items():\n",
    "    for i, doc_id in enumerate(a_doc_ids):\n",
    "        sponsor2docs[a][i] = id_dict[doc_id]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use spacy to preprocess files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 7s, sys: 7min 33s, total: 38min 41s\n",
      "Wall time: 19min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "processed_docs = []    \n",
    "for doc in nlp.pipe(docs, n_threads=6, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = processed_docs\n",
    "del processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/djiao1/anaconda3/lib/python3.6/site-packages/gensim-3.1.0-py3.6-macosx-10.9-x86_64.egg/gensim/models/phrases.py:431: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "# Filter out words that occur too frequently or too rarely.\n",
    "max_freq = 0.5\n",
    "min_wordcount = 20\n",
    "dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
    "\n",
    "_ = dictionary[0]  # This sort of \"initializes\" dictionary.id2token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize data.\n",
    "\n",
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sponsors: 103\n",
      "Number of unique tokens: 4877\n",
      "Number of documents: 2212\n"
     ]
    }
   ],
   "source": [
    "print('Number of sponsors: %d' % len(sponsor2docs))\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.18 s, sys: 655 ms, total: 9.83 s\n",
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import AuthorTopicModel\n",
    "%time model = AuthorTopicModel(corpus=corpus, num_topics=20, id2word=dictionary.id2token, \\\n",
    "                author2doc=sponsor2docs, chunksize=2000, passes=1, eval_every=0, \\\n",
    "                iterations=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57min 14s, sys: 4min 58s, total: 1h 2min 13s\n",
      "Wall time: 15min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_list = []\n",
    "for i in range(5):\n",
    "    model = AuthorTopicModel(corpus=corpus, num_topics=20, id2word=dictionary.id2token, \\\n",
    "                    author2doc=sponsor2docs, chunksize=2000, passes=100, gamma_threshold=1e-10, \\\n",
    "                    eval_every=0, iterations=1, random_state=i)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    tc = sum([t[1] for t in top_topics])\n",
    "    model_list.append((model, tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic coherence: -2.880e+01\n"
     ]
    }
   ],
   "source": [
    "model, tc = max(model_list, key=lambda x: x[1])\n",
    "print('Topic coherence: %.3e' %tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('s115-topic20.atmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AuthorTopicModel(num_terms=4877, num_topics=20, num_authors=103, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.035*\"plan\" + 0.027*\"tax\" + 0.021*\"taxable\" + 0.019*\"activity\" + 0.018*\"service\" + 0.017*\"income\" + 0.016*\"transfer\" + 0.014*\"business\" + 0.013*\"taxable_year\" + 0.012*\"excess\"'),\n",
       " (1,\n",
       "  '0.027*\"Fiscal Year 2017\" + 0.000*\"improvements\" + 0.000*\"in_addition\" + 0.000*\"reportsnot_later\" + 0.000*\"reportsnot\" + 0.000*\"the Committee on Appropriations of the House of Representatives\" + 0.000*\"the Committee on Appropriations of the Senate\" + 0.000*\"l\" + 0.000*\"gift\" + 0.000*\"in_general\"'),\n",
       " (2,\n",
       "  '0.008*\"program\" + 0.007*\"individual\" + 0.007*\"service\" + 0.006*\"national\" + 0.006*\"agency\" + 0.006*\"grant\" + 0.005*\"strike\" + 0.005*\"information\" + 0.005*\"public\" + 0.005*\"entity\"'),\n",
       " (3,\n",
       "  '0.072*\"fee\" + 0.068*\"drug\" + 0.049*\"fiscal\" + 0.046*\"fiscal_year\" + 0.029*\"food\" + 0.027*\"cosmetic\" + 0.026*\"drug_cosmetic\" + 0.026*\"federal_food\" + 0.025*\"the Federal Food, Drug\" + 0.024*\"strike\"'),\n",
       " (4,\n",
       "  '0.000*\"Fiscal Year 2017\" + 0.000*\"record\" + 0.000*\"strike\" + 0.000*\"department\" + 0.000*\"public\" + 0.000*\"defense\" + 0.000*\"fiscal_year\" + 0.000*\"person\" + 0.000*\"fiscal\" + 0.000*\"plan\"'),\n",
       " (5,\n",
       "  '0.012*\"program\" + 0.010*\"agency\" + 0.008*\"service\" + 0.007*\"rule\" + 0.007*\"strike\" + 0.007*\"department\" + 0.006*\"cost\" + 0.006*\"management\" + 0.006*\"energy\" + 0.006*\"committee\"'),\n",
       " (6,\n",
       "  '0.023*\"land\" + 0.016*\"identify\" + 0.016*\"tax\" + 0.015*\"foreign\" + 0.015*\"forest\" + 0.015*\"national\" + 0.015*\"income\" + 0.014*\"taxable\" + 0.014*\"know\" + 0.012*\"property\"'),\n",
       " (7,\n",
       "  '0.071*\"social\" + 0.060*\"impact\" + 0.054*\"partnership\" + 0.051*\"project\" + 0.045*\"outcome\" + 0.043*\"intervention\" + 0.035*\"state_local\" + 0.030*\"local\" + 0.026*\"carbon\" + 0.022*\"achieve\"'),\n",
       " (8,\n",
       "  '0.109*\"record\" + 0.096*\"juvenile\" + 0.066*\"seal\" + 0.063*\"offense\" + 0.060*\"court\" + 0.052*\"petition\" + 0.041*\"petitioner\" + 0.033*\"specie\" + 0.029*\"hearing\" + 0.020*\"arrest\"'),\n",
       " (9,\n",
       "  '0.000*\"wilderness\" + 0.000*\"approximately\" + 0.000*\"acre\" + 0.000*\"land\" + 0.000*\"national\" + 0.000*\"map\" + 0.000*\"forest\" + 0.000*\"approximately_acre\" + 0.000*\"program\" + 0.000*\"comprise\"'),\n",
       " (10,\n",
       "  '0.015*\"government\" + 0.011*\"alien\" + 0.009*\"department\" + 0.009*\"the United States\" + 0.009*\"security\" + 0.008*\"defense\" + 0.008*\"foreign\" + 0.008*\"russian\" + 0.007*\"person\" + 0.007*\"fund\"'),\n",
       " (11,\n",
       "  '0.000*\"wilderness\" + 0.000*\"land\" + 0.000*\"approximately\" + 0.000*\"forest\" + 0.000*\"map\" + 0.000*\"acre\" + 0.000*\"comprise\" + 0.000*\"national_forest\" + 0.000*\"creek\" + 0.000*\"taxable\"'),\n",
       " (12,\n",
       "  '0.038*\"indian\" + 0.026*\"indian_tribe\" + 0.026*\"tribe\" + 0.022*\"energy\" + 0.017*\"land\" + 0.015*\"alaska\" + 0.015*\"fund\" + 0.015*\"plan\" + 0.013*\"appeal\" + 0.012*\"program\"'),\n",
       " (13,\n",
       "  '0.000*\"wilderness\" + 0.000*\"approximately\" + 0.000*\"national\" + 0.000*\"map\" + 0.000*\"acre\" + 0.000*\"comprise\" + 0.000*\"forest\" + 0.000*\"identify\" + 0.000*\"land\" + 0.000*\"foreign\"'),\n",
       " (14,\n",
       "  '0.000*\"program\" + 0.000*\"strike\" + 0.000*\"fiscal_year\" + 0.000*\"national\" + 0.000*\"public\" + 0.000*\"energy\" + 0.000*\"fund\" + 0.000*\"fiscal\" + 0.000*\"fee\" + 0.000*\"service\"'),\n",
       " (15,\n",
       "  '0.000*\"public_law\" + 0.000*\"fiscal\" + 0.000*\"that\" + 0.000*\"provide_that\" + 0.000*\"appropriations\" + 0.000*\"fiscal_year\" + 0.000*\"committees\" + 0.000*\"available\" + 0.000*\"department\" + 0.000*\"fee\"'),\n",
       " (16,\n",
       "  '0.084*\"wilderness\" + 0.056*\"approximately\" + 0.054*\"acre\" + 0.039*\"comprise\" + 0.037*\"map\" + 0.028*\"approximately_acre\" + 0.026*\"creek\" + 0.024*\"forest\" + 0.022*\"national_forest\" + 0.020*\"mountain\"'),\n",
       " (17,\n",
       "  '0.011*\"emission\" + 0.010*\"fossil\" + 0.010*\"sexual\" + 0.010*\"individual\" + 0.010*\"product\" + 0.010*\"worker\" + 0.009*\"zero\" + 0.008*\"vehicle\" + 0.008*\"disability\" + 0.007*\"information\"'),\n",
       " (18,\n",
       "  '0.027*\"plan\" + 0.019*\"service\" + 0.016*\"care\" + 0.015*\"benefit\" + 0.011*\"provider\" + 0.011*\"climate\" + 0.011*\"community\" + 0.010*\"model\" + 0.010*\"individual\" + 0.010*\"medicare\"'),\n",
       " (19,\n",
       "  '0.034*\"ii\" + 0.031*\"chapter\" + 0.029*\"new\" + 0.026*\"heading\" + 0.024*\"numerical\" + 0.024*\"amend_insert\" + 0.023*\"follow_new\" + 0.021*\"ii_chapter\" + 0.018*\"no\" + 0.017*\"land\"')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def show_author(name):\n",
    "    print('\\n%s' % name)\n",
    "    print('Docs:', model.author2doc[name])\n",
    "    print('Topics:')\n",
    "    pprint([topic for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "E000285\n",
      "Docs: [1109, 2, 26, 43, 109, 99, 101, 102, 103, 200, 254, 294, 315, 316, 343, 355, 387, 569, 617, 640, 645, 655, 682, 719, 712, 886, 809, 885, 876, 906, 941, 972, 1022, 1066, 1107, 1164, 1175, 1174, 1331, 1299, 1342, 1347, 1377, 1381, 1399, 1409, 1412, 1444, 1448, 1459, 1460, 1496, 1486, 1507, 1524, 1525, 1527, 1537, 1545, 1602, 1671, 1681, 1711, 1712, 1749, 1800, 1856, 1892, 1920, 1921, 1935, 1946, 1993, 2004, 2078, 2146, 2185]\n",
      "Topics:\n",
      "[(2, 0.3728403769175701), (5, 0.12001910223605643), (6, 0.50704268494309901)]\n"
     ]
    }
   ],
   "source": [
    "show_author('E000285')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(sponsors\n",
    "         ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "610px",
    "right": "20px",
    "top": "163px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
